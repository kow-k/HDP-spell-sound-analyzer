{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d675eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U pyLDAvis\n",
    "#!pip install -U pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "049cdefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports\n",
    "import os, sys\n",
    "import pprint as pp\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa9515b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 一つ上の階層のファイルを見るように設定\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db72d709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_doc_size: 11\n",
      "min_doc_size: 3\n",
      "term_class: spell\n",
      "term_is_skippy: True\n",
      "max_gap_size: 9\n",
      "n_for_ngram: 4\n",
      "accent_status: \n",
      "term_type: sp_skippy4gram\n"
     ]
    }
   ],
   "source": [
    "## term settings\n",
    "term_classes        = [ 'spell', 'sound' ]\n",
    "term_class          = term_classes[0]\n",
    "ngram_is_inclusive  = True\n",
    "## doc settings\n",
    "max_doc_size        = 11\n",
    "min_doc_size        =  3\n",
    "print(f\"max_doc_size: {max_doc_size}\")\n",
    "print(f\"min_doc_size: {min_doc_size}\")\n",
    "### boundary handling\n",
    "add_boundary       = False\n",
    "boundary_mark      = \"#\"\n",
    "## term setting\n",
    "gap_mark           = \"…\"\n",
    "term_is_skippy     = True\n",
    "n_for_ngram        = 4\n",
    "max_gap_ratio      = 0.8\n",
    "max_gap_size       = round(max_doc_size * max_gap_ratio)\n",
    "print(f\"term_class: {term_class}\")\n",
    "print(f\"term_is_skippy: {term_is_skippy}\")\n",
    "print(f\"max_gap_size: {max_gap_size}\")\n",
    "print(f\"n_for_ngram: {n_for_ngram}\")\n",
    "### accent handling\n",
    "suppress_accents   = True\n",
    "accent_marks       = [ \"ˈ\", \"ˌ\" ] \n",
    "if term_class == 'sound':\n",
    "    if suppress_accents:\n",
    "        accent_status = \"-unaccented\"\n",
    "    else:\n",
    "        accent_stratus = \"-accented\"\n",
    "else:\n",
    "    accent_status = \"\"\n",
    "print(f\"accent_status: {accent_status}\")\n",
    "## define term_type\n",
    "if term_class == 'spell':\n",
    "    if term_is_skippy:\n",
    "        term_type = f\"sp_skippy{n_for_ngram}gram\"\n",
    "    else:\n",
    "        term_type = f\"sp_{n_for_ngram}gram\"\n",
    "else:\n",
    "    if term_is_skippy:\n",
    "        term_type = f\"sn_skippy{n_for_ngram}gram\"\n",
    "    else:\n",
    "        term_type = f\"sn_{n_for_ngram}gram\"\n",
    "## check\n",
    "print(f\"term_type: {term_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5384bce3-6007-4cb1-a925-8eb0cc970d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_lang_key: en_A_only\n",
      "target lang: English adj (WN) [en_A_only]\n",
      "target_class: \n"
     ]
    }
   ],
   "source": [
    "## target language\n",
    "## a key must be part of a file name \n",
    "target_lang_dict = {    'en_US' : 'English (US)',\n",
    "                        'en_UK' : 'English (UK)',\n",
    "                        'en_N_only' : 'English noun (WN)',\n",
    "                        'en_V_only' : 'English verb (WN)',\n",
    "                        'en_A_only' : 'English adj (WN)',\n",
    "                        'en_R_only' : 'English adv (WN)',\n",
    "                        'ar'    : 'Arabic',\n",
    "                        'de'    : 'German',\n",
    "                        'de_N_only' : 'German Nouns',\n",
    "                        'de_non_N_only' : 'German Non-nouns',\n",
    "                        'eo'    : 'Esperanto',\n",
    "                        'es_ES' : 'Spanish (Spain)',\n",
    "                        'es_MX' : 'Spanish (Mexico)',\n",
    "                        'fi'    : 'Finnish',\n",
    "                        'fr_FR' : 'French (France)',\n",
    "                        'fr_QC' : 'French (Quebec)',\n",
    "                        'is'    : 'Icelandic',\n",
    "                        'ir'    : 'Irish',\n",
    "                        'nl'    : 'Dutch',\n",
    "                        'ro'    : 'Romanian',\n",
    "                        'sw'    : 'Swahili' }\n",
    "## proper language selection\n",
    "target_lang_keys = [    'en_US', # 0\n",
    "                        'en_UK', # 1\n",
    "                        'en_N_only', # 2\n",
    "                        'en_V_only', # 3\n",
    "                        'en_A_only', # 4\n",
    "                        'en_R_only', # 5\n",
    "                        'ar', # 6\n",
    "                        'de', # 7\n",
    "                        'de_N_only', # 8\n",
    "                        'de_non_N_only', # 9\n",
    "                        'eo', 'es_ES', 'es_MX',\n",
    "                        'fi', 'fr_FR', 'fr_QC',\n",
    "                        'is', 'nl', 'ro', 'sw',\n",
    "                        'ir' # This lacks sound\n",
    "                    ]\n",
    "## check\n",
    "target_lang_key  = target_lang_keys[4]\n",
    "print(f\"target_lang_key: {target_lang_key}\")\n",
    "print(f\"target lang: {target_lang_dict[target_lang_key]} [{target_lang_key}]\")\n",
    "## target_attr [effective only for Irish]\n",
    "target_class = \"\"\n",
    "#target_class = None # This causes an unrediable error\n",
    "if target_lang_key == \"ir\":\n",
    "    target_classes = [ 'adjectives', 'nouns', 'verbs' ]\n",
    "    target_class = f\"-{target_classes[3]}\"\n",
    "print(f\"target_class: {target_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b147beca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LDA/HDP\n",
    "apply_term_filtering = True\n",
    "## The following parameters need to be relatively large to prevent \"Row sum not equal 1\" error\n",
    "term_minfreq       = 2\n",
    "## The following value is crucial to prevent \"Row sum not equal 1\" error\n",
    "abuse_threshold    = 0.04 # larger value selects shorter units, smaller value selects longer units\n",
    "min_bot_size       = 3\n",
    "# number of terms listed for a given topic\n",
    "n_terms_to_show    = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5be707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sampling\n",
    "source_sampling          = True\n",
    "source_sampling_rate     = 0.5\n",
    "source_sampling_max_size = 5000\n",
    "second_sampling          = False\n",
    "second_sampling_rate     = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6421f501-f201-4800-9cc0-2c9bd6833256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/irish/word-irish-adjectives-spell.csv',\n",
      " 'data/irish/word-irish-noun-phrases-spell.csv',\n",
      " 'data/irish/word-irish-nouns-spell.csv',\n",
      " 'data/irish/word-irish-possessives-spell.csv',\n",
      " 'data/irish/word-irish-prepositions-spell.csv',\n",
      " 'data/irish/word-irish-verbs-spell.csv',\n",
      " 'data/open-dict-ipa/data1/ar.csv.gz',\n",
      " 'data/open-dict-ipa/data1/de.csv.gz',\n",
      " 'data/open-dict-ipa/data1/en_UK.csv.gz',\n",
      " 'data/open-dict-ipa/data1/en_US.csv.gz',\n",
      " 'data/open-dict-ipa/data1/eo.csv.gz',\n",
      " 'data/open-dict-ipa/data1/es_ES.csv.gz',\n",
      " 'data/open-dict-ipa/data1/es_MX.csv.gz',\n",
      " 'data/open-dict-ipa/data1/fa.csv.gz',\n",
      " 'data/open-dict-ipa/data1/fi.csv.gz',\n",
      " 'data/open-dict-ipa/data1/fr_FR.csv.gz',\n",
      " 'data/open-dict-ipa/data1/fr_QC.csv.gz',\n",
      " 'data/open-dict-ipa/data1/is.csv.gz',\n",
      " 'data/open-dict-ipa/data1/ja.csv.gz',\n",
      " 'data/open-dict-ipa/data1/jam.csv.gz',\n",
      " 'data/open-dict-ipa/data1/ma.csv.gz',\n",
      " 'data/open-dict-ipa/data1/nb.csv.gz',\n",
      " 'data/open-dict-ipa/data1/nl.csv.gz',\n",
      " 'data/open-dict-ipa/data1/or.csv.gz',\n",
      " 'data/open-dict-ipa/data1/sv.csv.gz',\n",
      " 'data/open-dict-ipa/data1/sw.csv.gz',\n",
      " 'data/open-dict-ipa/data1/vi_C.csv.gz',\n",
      " 'data/open-dict-ipa/data1/vi_N.csv.gz',\n",
      " 'data/open-dict-ipa/data1/vi_S.csv.gz',\n",
      " 'data/open-dict-ipa/data1/yue.csv.gz',\n",
      " 'data/open-dict-ipa/data1/zh_hans.csv.gz',\n",
      " 'data/open-dict-ipa/data1/zh_hant.csv.gz',\n",
      " 'data/open-dict-ipa/data1a/de_N_only.csv.gz',\n",
      " 'data/open-dict-ipa/data1a/de_non_N_only.csv.gz',\n",
      " 'data/wn3/en_A_only.csv',\n",
      " 'data/wn3/en_N_only.csv',\n",
      " 'data/wn3/en_R_only.csv',\n",
      " 'data/wn3/en_V_only.csv']\n"
     ]
    }
   ],
   "source": [
    "## set target files\n",
    "import glob\n",
    "data_dir1     = \"data/open-dict-ipa/data1\"\n",
    "data_dir2     = \"data/open-dict-ipa/data1a\"\n",
    "data_dir3     = \"data/wn3\"\n",
    "data_dir4     = \"data/irish\"\n",
    "target_files  = glob.glob(f\"{data_dir1}/*\")\n",
    "target_files2 = glob.glob(f\"{data_dir2}/*\")\n",
    "target_files.extend(target_files2)\n",
    "target_files3 = glob.glob(f\"{data_dir3}/*\")\n",
    "target_files.extend(target_files3)\n",
    "target_files4 = glob.glob(f\"{data_dir4}/*\")\n",
    "target_files.extend(target_files4)\n",
    "#\n",
    "target_files = sorted([ file for file in target_files if \".csv\" in file ])\n",
    "pp.pprint(target_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f84ec7-4291-4a71-bd29-cca5515f9072",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get source data from files\n",
    "import pandas as pd\n",
    "import gzip\n",
    "#target_language_key = \"en_US\" # can be changed to get other languages\n",
    "#if target_class != \"\" or target_class is not None:\n",
    "if target_class != \"\":\n",
    "    target_file = [ f for f in target_files if target_lang_key in f and target_class in f ][0]\n",
    "else:\n",
    "    target_file = [ f for f in target_files if target_lang_key in f ][0]\n",
    "print(f\"processing: {target_file}\")\n",
    "##\n",
    "if target_lang_key == \"ir\":\n",
    "    col_names = ['spell', 'POS']\n",
    "else:\n",
    "    col_names = ['spell', 'sound']\n",
    "#\n",
    "if target_file.endswith(\".gz\"):\n",
    "    with gzip.open(target_file, \"rt\") as f:\n",
    "        raw_df = pd.read_csv(f, encoding = 'utf8', header = None, names = col_names )\n",
    "else:\n",
    "    with open(target_file, \"rt\") as f:\n",
    "        raw_df = pd.read_csv(f, encoding = 'utf8', header = None, names = col_names )\n",
    "## normalize characters\n",
    "raw_df['spell'] = raw_df['spell'].apply(lambda x: unicodedata.normalize('NFC', str(x)))\n",
    "## modify sound\n",
    "try:\n",
    "    sounds = raw_df['sound'].apply(lambda x: x.strip('/') )\n",
    "    sounds = [ x.split(\"/,\")[0] for x in sounds ] # picks up only the first of multiple entries\n",
    "    raw_df['sound'] = sounds\n",
    "except (AttributeError, KeyError):\n",
    "    pass\n",
    "#\n",
    "raw_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2e0e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "## source sampling\n",
    "len(raw_df)\n",
    "if source_sampling:\n",
    "\tprint(f\"source sampling applied\")\n",
    "\tif len(raw_df) >= source_sampling_max_size:\n",
    "\t\traw_df = raw_df.sample(source_sampling_max_size)\n",
    "\telse:\n",
    "\t\traw_df = raw_df.sample(round(len(raw_df) * source_sampling_rate))\n",
    "## remove accent marking\n",
    "if suppress_accents:\n",
    "\ttry:\n",
    "\t\traw_df['sound'] = raw_df['sound'].apply(lambda x: \"\".join([ y for y in list(x) if y not in accent_marks ]))\n",
    "\texcept KeyError:\n",
    "\t\tpass\n",
    "## add boudary marks\n",
    "if add_boundary:\n",
    "\traw_df['spell'] = raw_df['spell'].apply(lambda x: f\"{boundary_mark}{x}{boundary_mark}\")\n",
    "\ttry:\n",
    "\t\traw_df['sound'] = raw_df['sound'].apply(lambda x: f\"{boundary_mark}{x}{boundary_mark}\")\n",
    "\texcept KeyError:\n",
    "\t\tpass\n",
    "#\n",
    "print(raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4567ec72",
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate 1-grams for spell and sound\n",
    "## spell\n",
    "raw_df['sp_1gram'] = raw_df['spell'].apply(lambda x: list(str(x)))\n",
    "# add column of size\n",
    "raw_df['sp_size'] = raw_df['sp_1gram'].apply(lambda x: len(x))\n",
    "# add column of count of '-' inside\n",
    "raw_df['hyphen'] = raw_df['sp_1gram'].apply(lambda x: list(x).count(\"-\"))\n",
    "# add column of count of '.' inside\n",
    "raw_df['period'] = raw_df['sp_1gram'].apply(lambda x: list(x).count(\".\"))\n",
    "## sound\n",
    "# takes the first entry, removes '/' around\n",
    "try:\n",
    "    raw_df['sn_1gram'] = raw_df['sound'].apply(lambda x: list(x) )\n",
    "except (TypeError, KeyError):\n",
    "    pass\n",
    "# add column of size\n",
    "try:\n",
    "    raw_df['sn_size'] = raw_df['sn_1gram'].apply(lambda x: len(x))\n",
    "except KeyError:\n",
    "    pass\n",
    "## check\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ea51f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## filtering raw_data by size\n",
    "print(f\"term_type: {term_type}\")\n",
    "if \"sp_\" in term_type:\n",
    "    df_filtered = raw_df[ (raw_df['sp_size'] <= max_doc_size) & (raw_df['sp_size'] >= min_doc_size) & (raw_df['hyphen'] == 0) & (raw_df['period'] == 0) ]\n",
    "else:\n",
    "    df_filtered = raw_df[ (raw_df['sn_size'] <= max_doc_size) & (raw_df['sn_size'] >= min_doc_size) ]\n",
    "#\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768aebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define df after second sampling if any\n",
    "len(df_filtered)\n",
    "if second_sampling:\n",
    "    df = df_filtered.sample(round(len(df_filtered) * second_sampling_rate))\n",
    "else:\n",
    "    df = df_filtered\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2125b43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## spell 2grams\n",
    "#import ngrams\n",
    "import gen_ngrams\n",
    "module_name = \"gen_ngrams\"\n",
    "reload_module = False\n",
    "if reload_module:\n",
    "    import importlib\n",
    "    importlib.reload(module_name)\n",
    "\n",
    "if term_class == 'spell':\n",
    "    #sp_2grams = [ ngrams.list_gen_ngrams (x, n = 2, check = False) for x in df['sp_1gram'] ]\n",
    "    sp_2grams = [ gen_ngrams.gen_ngrams (x, n = 2, sep = \"\", check = False) for x in df['sp_1gram'] ]\n",
    "    if ngram_is_inclusive:\n",
    "        for i, g in enumerate(sp_2grams):\n",
    "            g.extend(list(df['sp_1gram'])[i])\n",
    "    ## add sp_2gram\n",
    "    df['sp_2gram'] = sp_2grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d632a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## spell 3grams\n",
    "#import ngrams\n",
    "import gen_ngrams\n",
    "if n_for_ngram > 2 and term_class == 'spell':\n",
    "    #sp_3grams = [ ngrams.list_gen_ngrams (x, n = 3, check = False) for x in df['sp_1gram'] ]\n",
    "    sp_3grams = [ gen_ngrams.gen_ngrams (x, n = 3, sep = \"\", check = False) for x in df['sp_1gram'] ]\n",
    "    if ngram_is_inclusive:\n",
    "        for i, g in enumerate(sp_3grams):\n",
    "            g.extend(list(df['sp_2gram'])[i])\n",
    "    ## add sp_3gram\n",
    "    df['sp_3gram'] = sp_3grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa2dd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## spell 4grams\n",
    "#import ngrams\n",
    "import gen_ngrams\n",
    "if n_for_ngram > 3 and term_class == 'spell':\n",
    "    #sp_4grams = [ ngrams.list_gen_ngrams (x, n = 4, check = False) for x in df['sp_1gram'] ]\n",
    "    sp_4grams = [ gen_ngrams.gen_ngrams (x, n = 4, sep = \"\", check = False) for x in df['sp_1gram'] ]\n",
    "    if ngram_is_inclusive:\n",
    "        for i, g in enumerate(sp_4grams):\n",
    "            g.extend(list(df['sp_3gram'])[i])\n",
    "    ## add sp_4gram\n",
    "    df['sp_4gram'] = sp_4grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a26cb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## spell 5grams\n",
    "#import ngrams\n",
    "import gen_ngrams\n",
    "if n_for_ngram > 4 and term_class == 'spell':\n",
    "    sp_5grams = [ gen_ngrams.gen_ngrams (x, n = 5, sep = \"\", check = False) for x in df['sp_1gram'] ]\n",
    "    if ngram_is_inclusive:\n",
    "        for i, g in enumerate(sp_5grams):\n",
    "            g.extend(list(df['sp_4gram'])[i])\n",
    "    ## add sp_5gram\n",
    "    df['sp_5gram'] = sp_5grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56279426",
   "metadata": {},
   "outputs": [],
   "source": [
    "## spell skippy 2gram\n",
    "#import ngrams_skippy\n",
    "import gen_ngrams\n",
    "reload_module = False\n",
    "module_name = \"gen_ngrams\"\n",
    "if reload_module:\n",
    "    import importlib\n",
    "    importlib.reload(module_name)\n",
    "#\n",
    "if term_class == 'spell':\n",
    "    #sp_skippy2grams = [ ngrams_skippy.gen_skippy2grams(x, missing_mark = gap_mark, check = False) for x in df['sp_1gram'] ]\n",
    "    ## The code above was replaced by the following more efficient one\n",
    "    sp_skippy2grams = [ gen_ngrams.gen_skippy_ngrams(x, 2, sep = \"\", max_distance = max_gap_size, missing_mark = gap_mark, check = False) for x in df['sp_1gram'] ]\n",
    "    if ngram_is_inclusive:\n",
    "        for i, g in enumerate(sp_skippy2grams):\n",
    "            g.extend(list(df['sp_1gram'])[i])\n",
    "    #\n",
    "    df['sp_skippy2gram'] = sp_skippy2grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc99e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## spell skippy 3gram\n",
    "#import ngrams_skippy\n",
    "import gen_ngrams\n",
    "if n_for_ngram > 2 and term_class == 'spell':\n",
    "    #sp_skippy3grams = [ ngrams_skippy.gen_skippy3grams(x, missing_mark = gap_mark, check = False) for x in df['sp_1gram'] ]\n",
    "    ## The code above was replaced by the following more efficient one\n",
    "    sp_skippy3grams = [ gen_ngrams.gen_skippy_ngrams(x, 3, sep = \"\", max_distance = max_gap_size, missing_mark = gap_mark, check = False) for x in df['sp_1gram'] ]\n",
    "    if ngram_is_inclusive:\n",
    "        for i, g in enumerate(sp_skippy3grams):\n",
    "            g.extend(list(df['sp_skippy2gram'])[i])\n",
    "    #\n",
    "    df['sp_skippy3gram'] = sp_skippy3grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfd7189",
   "metadata": {},
   "outputs": [],
   "source": [
    "## spell skippy 4gram\n",
    "#import ngrams_skippy\n",
    "import gen_ngrams\n",
    "if n_for_ngram > 3 and term_class == 'spell':\n",
    "    #sp_skippy4grams = [ ngrams_skippy.gen_skippy4grams(x, missing_mark = gap_mark, check = False) for x in df['sp_1gram'] ]\n",
    "    ## The code above was replaced by the following more efficient one\n",
    "    sp_skippy4grams = [ gen_ngrams.gen_skippy_ngrams(x, 4, sep = \"\", max_distance = max_gap_size, missing_mark = gap_mark, check = False) for x in df['sp_1gram'] ]\n",
    "    if ngram_is_inclusive:\n",
    "        for i, g in enumerate(sp_skippy4grams):\n",
    "            g.extend(list(df['sp_skippy3gram'])[i])\n",
    "    #\n",
    "    df['sp_skippy4gram'] = sp_skippy4grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0c00d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## spell skippy 5gram\n",
    "import gen_ngrams\n",
    "if n_for_ngram > 4 and term_class == 'spell':\n",
    "    sp_skippy5grams = [ gen_ngrams.gen_skippy_ngrams(x, 5, sep = \"\", max_distance = max_gap_size, missing_mark = gap_mark, check = False) for x in df['sp_1gram'] ]\n",
    "    if ngram_is_inclusive:\n",
    "        for i, g in enumerate(sp_skippy5grams):\n",
    "            g.extend(list(df['sp_skippy4gram'])[i])\n",
    "    #\n",
    "    df['sp_skippy5gram'] = sp_skippy5grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aec3ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sound 2grams\n",
    "#import ngrams\n",
    "import gen_ngrams\n",
    "module_name = \"gen_ngrams\"\n",
    "reload_module = False\n",
    "if reload_module:\n",
    "    import importlib\n",
    "    importlib.reload(module_name)\n",
    "#\n",
    "if term_class == 'sound':\n",
    "    #sn_2grams = [ ngrams.list_gen_ngrams (x, n = 2, check = False) for x in df['sn_1gram'] ]\n",
    "    sn_2grams = [ gen_ngrams.gen_ngrams(x, n = 2, sep =\"\", check = False) for x in df['sn_1gram'] ]\n",
    "    if ngram_is_inclusive:\n",
    "        for i, g in enumerate(sn_2grams):\n",
    "            g.extend(list(df['sn_1gram'])[i])\n",
    "    ## add sn_2gram\n",
    "    df['sn_2gram'] = sn_2grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d49502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sound 3grams\n",
    "#import ngrams\n",
    "import gen_ngrams\n",
    "if n_for_ngram > 2 and term_class == 'sound':\n",
    "    #sn_3grams = [ ngrams.list_gen_ngrams (x, n = 3, check = False) for x in df['sn_1gram'] ]\n",
    "    sn_3grams = [ gen_ngrams.gen_ngrams(x, n = 3, sep = \"\", check = False) for x in df['sn_1gram'] ]\n",
    "    if ngram_is_inclusive:\n",
    "        for i, g in enumerate(sn_3grams):\n",
    "            g.extend(list(df['sn_2gram'])[i])\n",
    "    ## add sn_3gram\n",
    "    df['sn_3gram'] = sn_3grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05411ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sound 4grams\n",
    "#import ngrams\n",
    "import gen_ngrams\n",
    "if n_for_ngram > 3 and term_class == 'sound':\n",
    "    #sn_4grams = [ ngrams.list_gen_ngrams (x, n = 4, check = False) for x in df['sn_1gram'] ]\n",
    "    sn_4grams = [ gen_ngrams.gen_ngrams (x, n = 4, sep = \"\", check = False) for x in df['sn_1gram'] ]\n",
    "    if ngram_is_inclusive:\n",
    "        for i, g in enumerate(sn_4grams):\n",
    "            g.extend(list(df['sn_3gram'])[i])\n",
    "    ## add sn_4gram\n",
    "    df['sn_4gram'] = sn_4grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3035d2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sound 5grams\n",
    "import gen_ngrams\n",
    "if n_for_ngram > 4 and term_class == 'sound':\n",
    "    sn_5grams = [ gen_ngrams.gen_ngrams (x, n = 5, sep = \"\", check = False) for x in df['sn_1gram'] ]\n",
    "    if ngram_is_inclusive:\n",
    "        for i, g in enumerate(sn_5grams):\n",
    "            g.extend(list(df['sn_4gram'])[i])\n",
    "    ## add sn_4gram\n",
    "    df['sn_5gram'] = sn_5grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa29f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sound skippy 2gram\n",
    "#import ngrams_skippy\n",
    "import gen_ngrams\n",
    "if term_class == 'sound':\n",
    "    #sn_skippy2grams = [ ngrams_skippy.gen_skippy2grams(x, missing_mark = gap_mark, check = False) for x in df['sn_1gram'] ]\n",
    "    ## The code above was replaced by the following more efficient one\n",
    "    sn_skippy2grams = [ gen_ngrams.gen_skippy_ngrams(x, n = 2, sep = \"\", max_distance = max_gap_size, missing_mark = gap_mark, check = False) for x in df['sn_1gram'] ]\n",
    "    if ngram_is_inclusive:\n",
    "        for i, g in enumerate(sn_skippy2grams):\n",
    "            g.extend(list(df['sn_1gram'])[i])\n",
    "    #\n",
    "    df['sn_skippy2gram'] = sn_skippy2grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32331bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sound skippy 3gram\n",
    "#import ngrams_skippy\n",
    "import gen_ngrams\n",
    "if n_for_ngram > 2 and term_class == 'sound':\n",
    "    #sn_skippy3grams = [ ngrams_skippy.gen_skippy3grams(x, missing_mark = gap_mark, check = False) for x in df['sn_1gram'] ]\n",
    "    ## The code above was replaced by the following more efficient one\n",
    "    sn_skippy3grams = [ gen_ngrams.gen_skippy_ngrams(x, n = 3, sep = \"\", max_distance = max_gap_size, missing_mark = gap_mark, check = False) for x in df['sn_1gram'] ]\n",
    "    if ngram_is_inclusive:\n",
    "        for i, g in enumerate(sn_skippy3grams):\n",
    "            g.extend(list(df['sn_skippy2gram'])[i])\n",
    "    #\n",
    "    df['sn_skippy3gram'] = sn_skippy3grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1599af",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sound skippy 4gram\n",
    "#import ngrams_skippy\n",
    "import gen_ngrams\n",
    "if n_for_ngram > 3 and term_class == 'sound':\n",
    "    #sn_skippy4grams = [ ngrams_skippy.gen_skippy4grams(x, missing_mark = gap_mark, check = False) for x in df['sn_1gram'] ]\n",
    "    ## The code above was replaced by the following more efficient one\n",
    "    sn_skippy4grams = [ gen_ngrams.gen_skippy_ngrams(x, n = 4, sep = \"\", max_distance = max_gap_size, missing_mark = gap_mark, check = False) for x in df['sn_1gram'] ]\n",
    "    if ngram_is_inclusive:\n",
    "        for i, g in enumerate(sn_skippy4grams):\n",
    "            g.extend(list(df['sn_skippy3gram'])[i])\n",
    "    #\n",
    "    df['sn_skippy4gram'] = sn_skippy4grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3363b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sound skippy 5gram\n",
    "import gen_ngrams\n",
    "if n_for_ngram > 4 and term_class == 'sound':\n",
    "    sn_skippy5grams = [ gen_ngrams.gen_skippy_ngrams(x, n = 5, sep = \"\", max_distance = max_gap_size, missing_mark = gap_mark, check = False) for x in df['sn_1gram'] ]\n",
    "    if ngram_is_inclusive:\n",
    "        for i, g in enumerate(sn_skippy5grams):\n",
    "            g.extend(list(df['sn_skippy4gram'])[i])\n",
    "    #\n",
    "    df['sn_skippy5gram'] = sn_skippy5grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eed700",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check df\n",
    "dropped_vars = [ 'sp_size', 'hyphen', 'period', 'sn_size' ]\n",
    "if term_class == 'spell':\n",
    "    extra = [ 'sn_1gram', 'sn_2gram', 'sn_3gram', 'sn_4gram',\n",
    "             'sn_skippy2gram', 'sn_skippy3gram', 'sn_skippy4gram' ]\n",
    "    dropped_vars.extend(extra)\n",
    "    target_vars = [ x for x in df.columns if not x in dropped_vars ]\n",
    "else:\n",
    "    extra = [ 'sp_1gram', 'sp_2gram', 'sp_3gram', 'sp_4gram',\n",
    "             'sp_skippy2gram', 'sp_skippy3gram', 'sp_skippy4gram' ]\n",
    "    dropped_vars.extend(extra)\n",
    "    target_vars = [ x for x in df.columns if not x in dropped_vars ]    \n",
    "#\n",
    "df[target_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7af298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## select data type and define doc_dict\n",
    "import random\n",
    "if \"sp_\" in term_type:\n",
    "    base_type = \"spell\"\n",
    "else:\n",
    "    base_type = \"sound\"\n",
    "doc_dict = { i: x for i, x in enumerate(df[base_type]) }\n",
    "## check\n",
    "random.sample(doc_dict.items(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd01681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## select bots for analysis\n",
    "enable_term_change = False # if you want to change term_type to save time and energy\n",
    "if enable_term_change:\n",
    "\tterm_type = 'sp_skippy4gram'\n",
    "print(f\"(changed) term_type: {term_type}\")\n",
    "\n",
    "## bot stands for 'bag-of-terms', a generalization of 'bag-of-words'\n",
    "bots = [ x for x in df[term_type] if len(x) > min_bot_size ] # Crucially\n",
    "import random\n",
    "random.sample(bots, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b338eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate dictionary\n",
    "from gensim.corpora import Dictionary\n",
    "diction = Dictionary(bots)\n",
    "print(diction)\n",
    "#apply_term_filtering = False\n",
    "if apply_term_filtering:\n",
    "    print(f\"term filtering applied\")\n",
    "    diction.filter_extremes(no_below = term_minfreq, no_above = abuse_threshold)\n",
    "else:\n",
    "    print(f\"term filtering not applied\")\n",
    "## check\n",
    "print(diction)\n",
    "## generate DTM\n",
    "corpus = [ diction.doc2bow(bot) for bot in bots if len(bot) > min_bot_size ] # Crucially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa772e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HDP (n_topics = 15)\n",
    "import gensim.models\n",
    "import pyLDAvis.gensim\n",
    "max_n_topics = 15\n",
    "hdp15 = gensim.models.HdpModel(corpus, diction, T = max_n_topics, random_state = 1)\n",
    "vis_data15 = pyLDAvis.gensim.prepare(hdp15, corpus, diction)\n",
    "pyLDAvis.display(vis_data15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cb8009",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save LDAvis output as a html file\n",
    "save_LDAvis = True\n",
    "if save_LDAvis:\n",
    "\tlang_dir_name = target_lang_dict[target_lang_key].split()[0]\n",
    "\tvis_output = f\"results/LDAvis/{lang_dir_name}/{target_lang_dict[target_lang_key]}{target_class}-HDP-max_ntop{max_n_topics}-{term_type}{accent_status}.html\"\n",
    "\tpyLDAvis.save_html(vis_data15, vis_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dab262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## topic investigation\n",
    "import numpy as np\n",
    "import HDP_helper\n",
    "reload_module = False\n",
    "if reload_module:\n",
    "    import importlib\n",
    "    importlib.reload(HDP_helper)\n",
    "\n",
    "documents_topics = np.zeros([hdp15.m_T, len(corpus)])\n",
    "for doc_id, c in enumerate(corpus):\n",
    "    for topic_id, prob in hdp15[c]:\n",
    "        documents_topics[topic_id][doc_id] = prob\n",
    "\n",
    "n_docs_to_show  = 10\n",
    "n_terms_to_pick = 60\n",
    "hdp15.optimal_ordering()\n",
    "for topic_id, probs in enumerate(documents_topics):\n",
    "    print(f\"==============\")\n",
    "    topic_t = hdp15.print_topic(topic_id, topn = n_terms_to_pick)\n",
    "    print(f\"topic_id {topic_id}: {HDP_helper.reformat_topic (topic_t, n_terms_to_pick)}\")\n",
    "    print(f\"nonzero count: \", len(probs.nonzero()[0]))\n",
    "    for doc_id in probs.argsort()[::-1][:n_docs_to_show]:\n",
    "        doc = doc_dict[doc_id]\n",
    "        print(f\"\\t{probs[doc_id]:0.4f}: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a897606f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save topic structures\n",
    "#hdp.get_topics() # =/= show_topics()\n",
    "#hdp.print_topics()\n",
    "hdp_topics = hdp15.show_topics(num_topics = max_n_topics,\n",
    "                               num_words = n_terms_to_show, formatted = False)\n",
    "hdp_dict = { tid: values for tid, values in hdp_topics }\n",
    "## convert to Pandas dataframe\n",
    "topics_df = pd.DataFrame.from_dict(hdp_dict)\n",
    "#hdp15_topics_out = f\"results/terms-by-topics-raw/hdp{max_n_topics}_topics_raw.csv\"\n",
    "hdp15_topics_out = f\"results/terms-by-topics-raw/{lang_dir_name}/{target_lang_dict[target_lang_key]}{target_class}-topics{max_n_topics}-{term_type}{accent_status}.csv\"\n",
    "topics_df.to_csv(hdp15_topics_out, header = False, index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ef367f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HDP (n_topics = 45)\n",
    "import gensim.models\n",
    "import pyLDAvis.gensim\n",
    "max_n_topics = 45\n",
    "hdp45 = gensim.models.HdpModel(corpus, diction, T = max_n_topics, random_state = 1)\n",
    "vis_data45 = pyLDAvis.gensim.prepare(hdp45, corpus, diction)\n",
    "pyLDAvis.display(vis_data45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db986e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save LDAvis output as a html file\n",
    "lang_dir_name = target_lang_dict[target_lang_key].split()[0]\n",
    "save_LDAvis = True\n",
    "if save_LDAvis:\n",
    "\tvis_output = f\"results/LDAvis/{lang_dir_name}/{target_lang_dict[target_lang_key]}{target_class}-HDP-max_ntop{max_n_topics}-{term_type}{accent_status}.html\"\n",
    "\tpyLDAvis.save_html(vis_data45, vis_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22fbf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save topic structures\n",
    "#hdp.get_topics() # =/= show_topics()\n",
    "#hdp.print_topics()\n",
    "hdp_topics = hdp45.show_topics(num_topics = max_n_topics,\n",
    "                               num_words = n_terms_to_show, formatted = False)\n",
    "hdp_dict = { tid: values for tid, values in hdp_topics }\n",
    "## convert to Pandas dataframe\n",
    "topics_df = pd.DataFrame.from_dict(hdp_dict)\n",
    "#hdp45_topics_out = f\"results/terms-by-topics-raw/hdp{max_n_topics}_topics_raw.csv\"\n",
    "hdp45_topics_out = f\"results/terms-by-topics-raw/{lang_dir_name}/{target_lang_dict[target_lang_key]}{target_class}-topics{max_n_topics}-{term_type}{accent_status}.csv\"\n",
    "topics_df.to_csv(hdp45_topics_out, header = False, index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c3001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## topic investigation\n",
    "import numpy as np\n",
    "import HDP_helper\n",
    "documents_topics = np.zeros([hdp45.m_T, len(corpus)])\n",
    "for doc_id, c in enumerate(corpus):\n",
    "    for topic_id, prob in hdp45[c]:\n",
    "        documents_topics[topic_id][doc_id] = prob\n",
    "#\n",
    "n_docs_to_show  = 10\n",
    "n_terms_to_pick = 10\n",
    "hdp45.optimal_ordering()\n",
    "for topic_id, probs in enumerate(documents_topics):\n",
    "    print(f\"==============\")\n",
    "    topic_t = hdp45.print_topic(topic_id, topn = n_terms_to_pick)\n",
    "    print(f\"topic_id {topic_id}: {HDP_helper.reformat_topic (topic_t, n_terms_to_pick)}\")\n",
    "    print(f\"nonzero count: {len(probs.nonzero()[0])}\")\n",
    "    for doc_id in probs.argsort()[::-1][:n_docs_to_show]:\n",
    "        doc = doc_dict[doc_id]\n",
    "        print(f\"\\t{probs[doc_id]:0.4f}: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755ef2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HDP (n_topics = 90)\n",
    "import gensim.models\n",
    "import pyLDAvis.gensim\n",
    "max_n_topics = 90\n",
    "hdp90 = gensim.models.HdpModel(corpus, diction, T = max_n_topics,\n",
    "                               random_state = 1,\n",
    "                               #var_converge = 0.001\n",
    "                               )\n",
    "vis_data90 = pyLDAvis.gensim.prepare(hdp90, corpus, diction)\n",
    "pyLDAvis.display(vis_data90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465c1b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save LDAvis output as a html file\n",
    "save_LDAvis = True\n",
    "if save_LDAvis:\n",
    "\tlang_dir_name = target_lang_dict[target_lang_key].split()[0]\n",
    "\tvis_output = f\"results/LDAvis/{lang_dir_name}/{target_lang_dict[target_lang_key]}{target_class}-HDP-max_ntop{max_n_topics}-{term_type}{accent_status}.html\"\n",
    "\tpyLDAvis.save_html(vis_data90, vis_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375d949d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save topic structures\n",
    "hdp_topics = hdp90.show_topics(num_topics = max_n_topics,\n",
    "                               num_words = n_terms_to_show, formatted = False)\n",
    "hdp_dict = { tid: values for tid, values in hdp_topics }\n",
    "## convert to Pandas dataframe\n",
    "topics_df = pd.DataFrame.from_dict(hdp_dict)\n",
    "#hdp90_topics_out = f\"results/terms-by-topics-raw/hdp{max_n_topics}_topics_raw.csv\"\n",
    "hdp90_topics_out = f\"results/terms-by-topics-raw/{lang_dir_name}/{target_lang_dict[target_lang_key]}{target_class}-topics{max_n_topics}-{term_type}{accent_status}.csv\"\n",
    "topics_df.to_csv(hdp90_topics_out, header = False, index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0e6525",
   "metadata": {},
   "outputs": [],
   "source": [
    "## topic investigation\n",
    "import numpy as np\n",
    "import HDP_helper\n",
    "documents_topics = np.zeros([hdp90.m_T, len(corpus)])\n",
    "for doc_id, c in enumerate(corpus):\n",
    "    for topic_id, prob in hdp90[c]:\n",
    "        documents_topics[topic_id][doc_id] = prob\n",
    "## investigate topics\n",
    "n_docs_to_show  = 10\n",
    "n_terms_to_pick = 10\n",
    "hdp90.optimal_ordering()\n",
    "for topic_id, probs in enumerate(documents_topics):\n",
    "    print(f\"==============\")\n",
    "    #topic_encoding = \", \".join(hdp.show_topic(topic_id))\n",
    "    topic_t = hdp90.print_topic(topic_id, topn = n_terms_to_pick)\n",
    "    print(f\"topic_id {topic_id}: {HDP_helper.reformat_topic (topic_t, n_terms_to_pick)}\")\n",
    "    print(f\"nonzero count: \", len(probs.nonzero()[0]))\n",
    "    for doc_id in probs.argsort()[::-1][:n_docs_to_show]:\n",
    "        doc = doc_dict[doc_id]\n",
    "        print(f\"\\t{probs[doc_id]:0.4f}: {doc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
