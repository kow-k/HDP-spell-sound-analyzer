{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53e9c557",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b56cb971",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base (Python 3.11.9)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#!conda update conda -y\n",
    "#!conda install Cython -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d675eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -U pandas\n",
    "#%pip install -U pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports\n",
    "import os, sys\n",
    "import pprint as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa9515b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 一つ上の階層のファイルを見るように設定\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e89a1f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cython module の生成 (必要に応じて)\n",
    "#!python clean setup.py build_ext --inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9395fca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cython を使うかどうか\n",
    "use_Cython = True\n",
    "if use_Cython:\n",
    "    %load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5384bce3-6007-4cb1-a925-8eb0cc970d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "## doc\n",
    "doc_max_size = 30 # max count of words in a sentence\n",
    "doc_min_size = 5  # min count of words in a sentence\n",
    "\n",
    "## term: w_skippy4gram often fails\n",
    "term_types    = [ 'w_1gram', 'w_2gram', 'w_3gram', 'w_4gram',\n",
    "                'w_skippy2gram', 'w_skippy3gram', 'w_skippy4gram' ]\n",
    "term_type     = term_types[-1]\n",
    "# n-gram\n",
    "ngram_is_inclusive  = True\n",
    "ngram_inclusiveness = 1 # When the value is k, n_gram contains (n-k)-grams\n",
    "## skippy n-grams\n",
    "gap_mark      = \"…\"\n",
    "max_gap_ratio = 0.23\n",
    "max_gap_val   = round(doc_max_size * max_gap_ratio)\n",
    "\n",
    "## check\n",
    "print(f\"term_type: {term_type}\")\n",
    "print(f\"max_gap_val for skippy n-gram: {max_gap_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0731c020",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HDP: The following parameters need to be relatively large for HDP, unlike LDA\n",
    "bot_min_size     = doc_min_size\n",
    "term_minfreq     = 3\n",
    "abuse_threshold  = 0.1\n",
    "n_docs_to_show   = 10\n",
    "n_terms_to_show  = 15\n",
    "# flags for HDP with larger topics\n",
    "explore_90_topics  = False\n",
    "explore_120_topics = False\n",
    "explore_150_topics = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c111a4c",
   "metadata": {},
   "source": [
    "# Get data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6421f501-f201-4800-9cc0-2c9bd6833256",
   "metadata": {},
   "outputs": [],
   "source": [
    "## variables\n",
    "random_target = False\n",
    "\n",
    "## Get target files\n",
    "import glob\n",
    "data_dir = \"data/Darwin-texts/single-lined/\"\n",
    "target_files = glob.glob(f\"{data_dir}/*\")\n",
    "target_files = [ file for file in target_files if \".txt\" in file ]\n",
    "pp.pprint(target_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f84ec7-4291-4a71-bd29-cca5515f9072",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read data from files\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "if random_target:\n",
    "    file = random.choice(target_files)\n",
    "else:\n",
    "    file = target_files[0]\n",
    "print(f\"processing: {file}\")\n",
    "#\n",
    "if file.endswith(\".csv\"):\n",
    "    with open(file, \"rt\") as f:\n",
    "        raw_df = pd.read_csv(f, encoding = 'utf8', header = None, names = ['sentence'])\n",
    "elif file.endswith(\".txt\"):\n",
    "    with open(file, \"rt\") as f:\n",
    "        raw_df = pd.read_table(f, encoding = 'utf8', header = None, names = ['sentence'])\n",
    "elif file.endswith(\".xlsx\"):\n",
    "    with open(file, \"rb\") as f:\n",
    "        raw_df = pd.read_excel(f, index_col = 0)\n",
    "# \n",
    "raw_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f85cdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## build w1gram\n",
    "import re\n",
    "\n",
    "w_1grams = raw_df['sentence'].apply(lambda x: re.split(r\"\\s+\", x))\n",
    "\n",
    "## convert to lowercase\n",
    "w_1grams = [ [ x.lower() for x in w1gram ] for w1gram in w_1grams ]\n",
    "\n",
    "## remove ineffective characters\n",
    "removed_chars = r\"[-.,:;!?()_\\\"\\'“”‘’]\"\n",
    "w_1grams = [ [ re.sub(removed_chars, \"\", x) for x in w_1gram ] for w_1gram in w_1grams ]\n",
    "\n",
    "## exclude single-character words\n",
    "#w1grams = [ [x for x in w1gram if len(x) > 1 ] for w1gram in w1grams ]\n",
    "[ words[:5] for words in random.sample(w_1grams, 3) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5194d389",
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove too frequent words\n",
    "from collections import Counter\n",
    "all_words = [ ]\n",
    "[ all_words.extend(x) for x in w_1grams ] \n",
    "word_counts = Counter(all_words)\n",
    "reduct_rate = 0.003 # needs to be optimized text-wise\n",
    "too_frequents = word_counts.most_common(round(len(word_counts) * reduct_rate))\n",
    "pp.pprint(too_frequents)\n",
    "print(f\"number of removed items: {len(too_frequents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0d675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## exclude too frequent words\n",
    "w_1grams = [ [ x for x in w1gram if not x in too_frequents ] for w1gram in w_1grams ]\n",
    "raw_df['w_1gram'] = w_1grams\n",
    "raw_df['size'] = raw_df['w_1gram'].apply(lambda x: len(x))\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacefd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define df by filtering by length\n",
    "print(f\"originally: {len(raw_df)}\")\n",
    "df = raw_df[ (doc_min_size <= raw_df['size']) & (raw_df['size'] <= doc_max_size) ]\n",
    "print(f\"after filtering: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fc9629",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fa9584",
   "metadata": {},
   "source": [
    "# Build n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b59585ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## generic function for n-gram generation: for words, seg_joint = \" \"\n",
    "reload_module_on_run = False\n",
    "if reload_module_on_run:\n",
    "    import importlib\n",
    "def add_ngram_to_df(dfx, n_for_ngram: int, seg_joint: str = \" \", var_prefix: str = \"\", ngram_is_skippy: bool = False, ngram_is_inclusive: bool = ngram_is_inclusive, ngram_inclusiveness: int = ngram_inclusiveness, use_Cython: bool = use_Cython, check: bool = False):\n",
    "    \"\"\"\n",
    "    generic function for adding n-gram column to df with a specified n for ngram\n",
    "    \"\"\"\n",
    "    print(f\"use_Cython: {use_Cython}\")\n",
    "    inclusion_size = (n_for_ngram - ngram_inclusiveness)\n",
    "    print(f\"inclusion_size: {inclusion_size}\")\n",
    "    assert inclusion_size >= 0\n",
    "    source_var = f\"{var_prefix}1gram\"\n",
    "    print(f\"source_var: {source_var}\")\n",
    "    unigrams = df[source_var]\n",
    "    if use_Cython:\n",
    "        import cy_gen_ngrams\n",
    "        if reload_module_on_run:\n",
    "            importlib.reload(cy_gen_ngrams)\n",
    "        if ngram_is_skippy:\n",
    "            ngrams = [ [seg_joint.join(x) for x in cy_gen_ngrams.cy_gen_skippy_ngrams(x, n = n_for_ngram, check = False)] for x in unigrams ]\n",
    "        else:\n",
    "            ngrams = [ [seg_joint.join(x) for x in cy_gen_ngrams.cy_gen_ngrams(x, n = n_for_ngram, check = False)] for x in unigrams ]\n",
    "    else:\n",
    "        import gen_ngrams\n",
    "        if ngram_is_skippy:\n",
    "            ngrams = [ gen_ngrams.gen_skippy_ngrams(x, n = n_for_ngram, sep = seg_joint, check = False) for x in unigrams ]\n",
    "        else:\n",
    "            ngrams = [ gen_ngrams.gen_ngrams(x, n = n_for_ngram, sep = seg_joint, check = False) for x in unigrams ]\n",
    "    ## 包括的 2gramの生成\n",
    "    if ngram_is_inclusive:\n",
    "        assert (n_for_ngram - inclusion_size) > 0\n",
    "        if ngram_is_skippy and n_for_ngram > 2:\n",
    "            supplement_var = f\"{var_prefix}skippy{n_for_ngram - 1}gram\"\n",
    "        else:\n",
    "            supplement_var = f\"{var_prefix}{n_for_ngram - 1}gram\"\n",
    "        print(f\"supplement_var: {supplement_var}\")\n",
    "        for i, g in enumerate(ngrams):\n",
    "            included = [ x for x in list(dfx[supplement_var])[i] if len(x) >= inclusion_size ]\n",
    "            if len(included) > 0:\n",
    "                g.extend(included)\n",
    "    ## 変数の追加\n",
    "    if ngram_is_skippy:\n",
    "        added_var = f\"{var_prefix}skippy{n_for_ngram}gram\"\n",
    "    else:\n",
    "        added_var = f\"{var_prefix}{n_for_ngram}gram\"\n",
    "    print(f\"added_var: {added_var}\")\n",
    "    dfx[added_var] = ngrams\n",
    "    ## check result\n",
    "    print(dfx[added_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fe8ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## word 2grams\n",
    "add_ngram_to_df(df, n_for_ngram = 2, var_prefix = \"w_\", seg_joint = \" \", ngram_is_skippy = False, ngram_is_inclusive = True, check = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284bc99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## word 3grams\n",
    "add_ngram_to_df(df, n_for_ngram = 3, var_prefix = \"w_\", seg_joint = \" \", ngram_is_skippy = False, ngram_is_inclusive = True, check = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e110a2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## word 4grams\n",
    "add_ngram_to_df(df, n_for_ngram = 4, var_prefix = \"w_\", seg_joint = \" \", ngram_is_skippy = False, ngram_is_inclusive = True, check = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6422ba02",
   "metadata": {},
   "outputs": [],
   "source": [
    "## word skippy 2grams\n",
    "add_ngram_to_df(df, n_for_ngram = 2, var_prefix = \"w_\", seg_joint = \" \", ngram_is_skippy = True, ngram_is_inclusive = True, check = False) # For words, seg_joint = \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e9fb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "## word skippy 3grams\n",
    "add_ngram_to_df(df, n_for_ngram = 3, var_prefix = \"w_\", seg_joint = \" \", ngram_is_skippy = True, ngram_is_inclusive = True, check = False) # For words, seg_joint = \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da17e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "## word skippy 4grams\n",
    "if term_type == \"w_skippy4gram\":\n",
    "    add_ngram_to_df(df, n_for_ngram = 4, var_prefix = \"w_\", seg_joint = \" \", ngram_is_skippy = True, ngram_is_inclusive = True, check = False) # For words, seg_joint = \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c8296e",
   "metadata": {},
   "source": [
    "# DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "76328dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcf3113",
   "metadata": {},
   "outputs": [],
   "source": [
    "## build doc_dict\n",
    "doc_dict = { i : x for i, x in enumerate(df['sentence']) }\n",
    "pp.pprint(random.sample(list(doc_dict.items()), 5)) # list(...) is needed after 3.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82092e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## select bots for DTM\n",
    "print(f\"term_type: {term_type}\")\n",
    "bots = list(df[term_type])\n",
    "bots = [ bot for bot in bots if len(bot) >= bot_min_size ]\n",
    "random.sample(bots, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ea51f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## build diction, corpus = dtm\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "## dtm\n",
    "diction = Dictionary(bots)\n",
    "print(diction)\n",
    "\n",
    "## filtering\n",
    "diction.filter_extremes(no_below = term_minfreq, no_above = abuse_threshold)\n",
    "print(diction)\n",
    "\n",
    "## corpus building: allow_update prevents \"Not all rows ...\" errror but it takes considerably longer.\n",
    "## Sanitization with nonzero filtering is more effective.\n",
    "#corpus = [ diction.doc2bow(bot, allow_update = True) for bot in bots ]\n",
    "corpus = [ diction.doc2bow(bot) for bot in bots if len(bot) >= doc_min_size ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3012d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sanitize corpus: Crucial for HDP\n",
    "original_size = len(corpus)\n",
    "corpus = [ doc for doc in corpus if len(doc) > 0 ]\n",
    "print(f\"discarded {original_size - len(corpus)} docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fade2fe5",
   "metadata": {},
   "source": [
    "# Run HDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a44c5c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "## topic investigation のutility function\n",
    "def investigate_topics(target_hdp, n_docs_to_show: int = n_docs_to_show,\n",
    "                       n_terms_to_show: int = n_terms_to_show, precision: float = 0.4):\n",
    "    import numpy as np\n",
    "    import HDP_helper\n",
    "\n",
    "    ## collect valid date\n",
    "    documents_topics = np.zeros([target_hdp.m_T, len(corpus)])\n",
    "    for doc_id, c in enumerate(corpus):\n",
    "        for topic_id, prob in target_hdp[c]:\n",
    "            documents_topics[topic_id][doc_id] = prob\n",
    "    \n",
    "    ## investigate topics\n",
    "    target_hdp.optimal_ordering()\n",
    "    for topic_id, probs in enumerate(documents_topics):\n",
    "        print(f\"==============\")\n",
    "        topic_t = target_hdp.print_topic(topic_id, topn = n_terms_to_show)\n",
    "        print(f\"topic_id {topic_id}: {HDP_helper.reformat_topic (topic_t, n_terms_to_show)}\")\n",
    "        print(f\"nonzero count: \", len(probs.nonzero()[0]))\n",
    "        for doc_id in probs.argsort()[::-1][:n_docs_to_show]:\n",
    "            doc = doc_dict[doc_id]\n",
    "            print(f\"\\t{probs[doc_id]:{precision}f}: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0900f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HDP (max_n_topics = 15)\n",
    "import gensim.models\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "max_n_topics = 15\n",
    "hdp15 = gensim.models.HdpModel(corpus, diction, random_state = 1, T = max_n_topics)\n",
    "vis_data15 = pyLDAvis.gensim.prepare(hdp15, corpus, diction)\n",
    "pyLDAvis.display(vis_data15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a430e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## topic investigation\n",
    "investigate_topics(hdp15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ef367f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HDP (max_n_topics = 45)\n",
    "import gensim.models\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "max_n_topics = 45\n",
    "hdp45 = gensim.models.HdpModel(corpus, diction, random_state = 1, T = max_n_topics)\n",
    "vis_data45 = pyLDAvis.gensim.prepare(hdp45, corpus, diction)\n",
    "pyLDAvis.display(vis_data45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d432729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## topic investigation\n",
    "investigate_topics(hdp45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "453ba270",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HDP (max_n_topics = 90)\n",
    "if explore_90_topics:\n",
    "    import gensim.models\n",
    "    import pyLDAvis.gensim\n",
    "\n",
    "    max_n_topics = 90\n",
    "    hdp90 = gensim.models.HdpModel(corpus, diction, random_state = 1, T = max_n_topics)\n",
    "    vis_data90 = pyLDAvis.gensim.prepare(hdp90, corpus, diction)\n",
    "    pyLDAvis.display(vis_data90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7eae4e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "## topic investigation\n",
    "if explore_90_topics:\n",
    "    investigate_topics(hdp90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "130f219b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HDP (max_n_topics = 120)\n",
    "if explore_120_topics:\n",
    "    import gensim.models\n",
    "    import pyLDAvis.gensim\n",
    "\n",
    "    max_n_topics = 120\n",
    "    hdp120 = gensim.models.HdpModel(corpus, diction, random_state = 1, T = max_n_topics)\n",
    "    vis_data120 = pyLDAvis.gensim.prepare(hdp120, corpus, diction)\n",
    "    pyLDAvis.display(vis_data120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6a431efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## topic investigation\n",
    "if explore_120_topics:\n",
    "    investigate_topics(hdp120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6195bc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HDP (max_n_topics = 150)\n",
    "if explore_150_topics:\n",
    "    import numpy as np\n",
    "    import gensim.models\n",
    "    import pyLDAvis.gensim\n",
    "\n",
    "    max_n_topics = 150\n",
    "    hdp150 = gensim.models.HdpModel(corpus, diction, T = max_n_topics, random_state = 1)\n",
    "    vis_data150 = pyLDAvis.gensim.prepare(hdp150, corpus, diction)\n",
    "    pyLDAvis.display(vis_data150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7acb8bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## topic investigation\n",
    "if explore_150_topics:\n",
    "    investigate_topics(hdp150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6b9898",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save LDAvis output as a html file\n",
    "save_LDAvis = True\n",
    "ntops = [ 15, 45, 90, 120, 150 ]\n",
    "vis_targets = [ f\"vis_data{ntop}\" for ntop in ntops ]\n",
    "print(f\"vis_targets: {vis_targets}\")\n",
    "vis_target_data = vis_targets[0]\n",
    "print(f\"vis_target_data: {vis_target_data}\")\n",
    "if save_LDAvis:\n",
    "\toutput = f\"results/LDAvis/Darwin-HDP-max_ntop{max_n_topics}-{term_type}.html\"\n",
    "\tpyLDAvis.save_html(eval(vis_target_data), output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
